{"name":"Ar-pagerank","tagline":"","body":"\r\n\r\n# Abstrakcja równoległa RDD\r\n\r\n* RDD: Resilient Distributed Dataset\r\n* https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf\r\n\r\n---\r\n\r\n# Apache Spark - instalacja i konfiguracja\r\n\r\n* https://spark.apache.org/downloads.html\r\n* Spark 1.0.2\r\n* Pre-build for Hadoop 2.2\r\n* http://d3kbcqa49mib13.cloudfront.net/spark-1.0.2-bin-hadoop2.tgz\r\n\r\n---\r\n\r\n# Uruchamianie na Zeusie\r\n\r\n* Należy zgłosić się do grupy `plgg-spark` przy pomocy portalu PL-Grid: https://portal.plgrid.pl/web/guest/teams\r\n* Instrukcja: https://docs.cyfronet.pl/display/~plgmyco/Hadoop+and+Spark+with+Cyfronet+PBS\r\n* Ustawienie środowiska dla zadań Spark:\r\n```bash\r\nsource $PLG_GROUPS_STORAGE/plgg-spark/set_env_spark-1.0.0.sh\r\n```\r\n* Uruchomienie interaktywnej konsoli Spark:\r\n```bash\r\nqsub -I -q plgrid-testing -l nodes=1:ppn=12 $SPARK_HOME/bin/spark-shell\r\n```\r\n\r\n\r\n---\r\n\r\n# RDD - przykład lista\r\n\r\n* Tablica\r\n```scala\r\nscala> val listData = Array(1,3,2,1,3,1,3,4,4,1,4,2)\r\n  \r\n  listData: Array[Int] = Array(1, 3, 2, 1, 3, 1, 3, 4, 4, 1, 4, 2)\r\n```\r\n* RDD representujące listę\r\n```scala\r\n  scala> val listRDD = sc.parallelize(listData)\r\n\r\n  listRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[9] at parallelize at <console>:14\r\n```\r\n---\r\n\r\n# RDD - przykład operacje na listach\r\n\r\n* Operacja `map()`\r\n```scala\r\n  scala> val squares = listRDD.map(x => x*x)\r\n\r\n  squares: org.apache.spark.rdd.RDD[Int] = MappedRDD[10] at map at <console>:16\r\n```\r\n\r\n* Przekształcenie w tablicę (np. do wypisania)\r\n```scala\r\n  scala> squares.collect\r\n\r\n  res5: Array[Int] = Array(1, 9, 4, 1, 9, 1, 9, 16, 16, 1, 16, 4)\r\n```\r\n* Operacja `reduce()`\r\n```scala\r\n  scala> squares.reduce(_+_)\r\n  res6: Int = 87\r\n```\r\n\r\n---\r\n\r\n# RDD - przykład plik\r\n\r\n* Wczytanie pliku\r\n```scala\r\n  scala> val textFile = sc.textFile(\"/etc/passwd\")\r\n\r\n  textFile: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at <console>:12\r\n```\r\n* Ilość linii\r\n```scala\r\n  scala>textFile.count() // Number of items in this RDD\r\n\r\n  res1: Long = 41\r\n```\r\n\r\n---\r\n\r\n# RDD - przykład linie w pliku\r\n\r\n\r\n* Filtrowanie linii zawierających `bash`\r\n```scala\r\n  scala> val linesWithBash = textFile.filter(line => line.contains(\"bash\"))\r\n\r\n  linesWithBash: org.apache.spark.rdd.RDD[String] = FilteredRDD[4] at filter at <console>:14\r\n```\r\n* Zliczanie linii zawierających `bash`\r\n```scala\r\n  scala> linesWithBash.count\r\n  \r\n  res2: Long = 3\r\n```\r\n\r\n---\r\n# PageRank\r\n\r\n* Brin, S.; Page, L. (1998). \"The anatomy of a large-scale hypertextual Web search engine\". Computer Networks and ISDN Systems 30: 107–117. doi:10.1016/S0169-7552(98)00110-X. ISSN 0169-7552. http://infolab.stanford.edu/pub/papers/google.pdf\r\n* Ian Rogers, Pagerank Explained Correctly with Examples: http://www.cs.princeton.edu/~chazelle/courses/BIB/pagerank.htm\r\n* `PR(A) = (1-d) + d (PR(T1)/C(T1) + ... + PR(Tn)/C(Tn))`\r\n* `d = 0.85`\r\n\r\n![PageRank cartoon](http://upload.wikimedia.org/wikipedia/commons/thumb/6/69/PageRank-hi-res.png/320px-PageRank-hi-res.png)\r\n\r\n\r\n---\r\n# PageRank - algorytm\r\n\r\n1. Każdy węzeł zaczyna z `PR = 1`\r\n2. W każdym kroku węzeł `p` przekazuje `PR(p)/C(p)` swoim sąsiadom, gdzie `C(p)` to ilość sąsiadów\r\n3. Nowa wartość `PR = 0.15 + 0.85 * Sum(contribs)`\r\n\r\n![PageRank](PageRank.png)\r\n\r\nhttp://ampcamp.berkeley.edu/wp-content/uploads/2012/06/matei-zaharia-part-2-amp-camp-2012-standalone-programs.pdf\r\n\r\n---\r\n\r\n# PageRank - przykład (1)\r\n\r\n```scala\r\nimport org.apache.spark.SparkContext\r\nimport org.apache.spark.SparkContext._\r\nimport org.apache.spark.SparkConf\r\n\r\nobject SimplePageRank {\r\n  def main(args: Array[String]) {\r\n    val ITERATIONS = 10\r\n\r\n    val conf = new SparkConf().setAppName(\"Simple PageRank\")\r\n    val sc = new SparkContext(conf)\r\n```    \r\n---\r\n\r\n# PageRank - przykład (2)\r\n\r\n```scala\r\n    //  Prepare data\r\n    val linksData = Array((\"a\",\"c\"),(\"b\",\"a\"),(\"c\",\"a\"),(\"c\",\"d\"),(\"d\",\"a\"),(\"d\",\"b\"))\r\n\r\n    //  RDD of (url, url) pairs\r\n    //  RDD[(String, String)]\r\n    val linksRDD = sc.parallelize(linksData)\r\n\r\n    //  RDD of (url, neighbors) pairs\r\n    //  RDD[(String, Iterable[String])]\r\n    val links = linksRDD.distinct().groupByKey().cache()\r\n\r\n    // RDD of (url, rank) pairs\r\n    // RDD[(String, Double)]\r\n    // Pass each value in the key-value pair RDD through a map function without changing the keys; \r\n    // this also retains the original RDD's partitioning.\r\n    var ranks = links.mapValues(v => 1.0)   \r\n```\r\n\r\n---\r\n\r\n# PageRank - przykład (3)\r\n\r\n```scala\r\n    for (i <- 1 to ITERATIONS) {\r\n      val contribs = links.join(ranks).values.flatMap {\r\n        case (urls, rank) =>\r\n          val size = urls.size\r\n          urls.map(url => (url, rank / size))\r\n      }\r\n      ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _)\r\n    }\r\n\r\n    // Return an array that contains all of the elements in this RDD.\r\n    val output = ranks.collect()\r\n\r\n    output.foreach(tup => println(tup._1 + \" has rank: \" + tup._2 + \".\"))\r\n\r\n    sc.stop()\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n# PageRank - przykład z wczytywaniem z pliku:\r\n\r\n```scala\r\npackage org.apache.spark.examples\r\n\r\nimport org.apache.spark.SparkContext._\r\nimport org.apache.spark.{SparkConf, SparkContext}\r\n\r\nobject SparkPageRank {\r\n  def main(args: Array[String]) {\r\n    if (args.length < 1) {\r\n      System.err.println(\"Usage: SparkPageRank <file> <iter>\")\r\n      System.exit(1)\r\n    }\r\n    val sparkConf = new SparkConf().setAppName(\"PageRank\")\r\n    val iters = if (args.length > 0) args(1).toInt else 10\r\n    val ctx = new SparkContext(sparkConf)\r\n    val lines = ctx.textFile(args(0), 1)\r\n    val links = lines.map{ s =>\r\n      val parts = s.split(\"\\\\s+\")\r\n      (parts(0), parts(1))\r\n    }.distinct().groupByKey().cache()\r\n```\r\n\r\nhttps://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}